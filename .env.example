# LLM model name - must include provider prefix for LiteLLM
# Format: provider/model-name
# Examples:
#   anthropic/claude-3-5-haiku-latest
#   anthropic/claude-3-5-sonnet-latest
#   anthropic/claude-sonnet-4-20250514
#   anthropic/bedrock-sonnet-4-5
#   openai/gpt-4o
LLM_NAME=anthropic/claude-3-5-haiku-latest

# Path to the repository to modify (local clone)
REPO_PATH=awx

# API Key (used by LiteLLM to authenticate with the provider)
ANTHROPIC_API_KEY=your-api-key-here

# Optional: Custom API base URL (e.g. for a proxy)
# ANTHROPIC_API_URL=https://your-proxy-url.example.com

# Lines per chunk when reading or writing large files (>CHUNK_SIZE lines).
# Optimal values by model:
#   claude-haiku-*    : 200  (smaller context budget)
#   claude-sonnet-4-5 : 500  (200 K-token context, ~13 K tokens per chunk)
#   claude-opus-*     : 500  (same large context)
# Default: 500
# FILE_CHUNK_SIZE=500
